{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVDI5YGBc14g1a84vyXVWz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["### ----------------------------------------------------------------------- ###\n","### ----------------------------- BIBLIOTECAS ----------------------------- ###\n","### ----------------------------------------------------------------------- ###\n","import os\n","import sys\n","import torch\n","import pandas as pd\n","import subprocess\n","\n","from flask import Flask, request, render_template, session, flash, redirect, url_for, jsonify\n","import logging as py_logging\n","import ngrok\n","from celery_utils import celery_init_app\n","\n","from datasets import load_dataset, Dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, logging\n","from peft import LoraConfig, PeftModel, get_peft_model\n","from trl import SFTTrainer\n","\n","from transformers import pipeline\n","from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n","from langchain_core.prompts import PromptTemplate\n","from langchain.chains import LLMChain, ConversationChain\n","from langchain.memory import ConversationBufferMemory"],"metadata":{"id":"FbqURDCuKoRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ----------------------------------------------------------------------- ###\n","### -------------------- CONFIGURACION DE LA APP FLASK -------------------- ###\n","### ----------------------------------------------------------------------- ###\n","app = Flask(__name__, template_folder='./templates')\n","\n","app.config.from_mapping(\n","    CELERY=dict(\n","        broker_url=\"redis://localhost:6379\",\n","        result_backend=\"redis://localhost:6379\",\n","        task_ignore_result=True,\n","    ),\n",")\n","celery = celery_init_app(app)\n","\n","activate_listener = 0\n","\n","if len(sys.argv) > 1:\n","    try:\n","        activate_listener = int(sys.argv[1])\n","    except ValueError:\n","        print(\"El valor de activate_listener debe ser un número entero.\")\n","\n","if (activate_listener == 1):\n","    os.environ['NGROK_AUTHTOKEN'] = 'NGROK_AUTHTOKEN_HERE'\n","    py_logging.basicConfig(level=py_logging.INFO)\n","    listener = ngrok.werkzeug_develop()\n","\n","tokenizer = None\n","model = None\n","pipe = None\n","llm = None\n","memory = None\n","template_conversation = None\n","prompt_conversation = None\n","conversation = None"],"metadata":{"id":"Z6esbFJKfgSp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_csv(row, name_col_question, name_col_answer):\n","  \"\"\" Esta funcion se encarga de ajustar el dataset enviado por el usuario al\n","  formato csv adecuado para poder convertir el csv en una cadena de texto. El\n","  csv de salida cuenta con una unica columna, nombrada 'text', donde cada\n","  muestra contiene los prompts de las diferentes preguntas y respuestas que se\n","  utilizan para entrenar el modelo. \"\"\"\n","\n","  boundary = \"--boundary--\"\n","  prompt = \"Below is an instruction that describes a task paired with input that provides further context. Write a response that appropriately completes the request.\"\n","  instruction = \"Answer the following question.\"\n","  question = str(row[name_col_question])\n","  answer = str(row[name_col_answer])\n","\n","  text = boundary + prompt + \"\\n\\n### Instruction:\\n\" + instruction + \"\\n\\n### Input:\\n\" + question + \"\\n\\n### Response:\\n\" + answer + \"</s>\"\n","\n","  adjust_ds_csv = pd.Series([text])\n","\n","  return adjust_ds_csv"],"metadata":{"id":"TlANCHJtCH9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def csv_to_string(ds):\n","  \"\"\" Esta funcion se encarga de convertir el csv introducido por el usuario a\n","  una unica cadena de texto. \"\"\"\n","\n","  adjust_ds_csv = ds[[ds.columns[0],ds.columns[1]]].apply(adjust_csv, args=(ds.columns[0], ds.columns[1]), axis=1)\n","\n","  list_of_samples_str = adjust_ds_csv.iloc[:, 0].astype(str)\n","  adjust_ds_string = ' '.join(list_of_samples_str)\n","\n","  return adjust_ds_string"],"metadata":{"id":"wVtiJN-wCK7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def string_to_csv(adjust_ds_string):\n","  \"\"\" Esta funcion se encarga de convertir el dataset en formato texto a\n","  formato csv. \"\"\"\n","\n","  samples = adjust_ds_string.split(\"--boundary--\")[1:]\n","  ds_from_strings = pd.DataFrame(samples, columns=['text'])\n","\n","  return ds_from_strings"],"metadata":{"id":"hEf7Z358CMuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ----------------------------------------------------------------------- ###\n","### --------------------- FUNCIONAMIENTO DE LA APP WEB -------------------- ###\n","### ----------------------------------------------------------------------- ###\n","@celery.task(bind=True)\n","def fine_tune_llm(self, adjust_ds_string, fine_tuned_model_name):\n","    \"\"\" Esta funcion se encarga del fine-tuning. Mientras se realiza el ajuste,\n","    se envian mensajes sobre el estado del proceso al cliente. \"\"\"\n","\n","\n","    message = \"Preparando el ajuste...\"\n","    self.update_state(state='PROGRESS', meta={'current': 0, 'total': 100, 'status': message})\n","\n","\n","    message = \"Adaptando el conjunto de datos...\"\n","    self.update_state(state='PROGRESS', meta={'current': 10, 'total': 100, 'status': message})\n","\n","    ds_from_strings = string_to_csv(adjust_ds_string)\n","    ds = Dataset.from_pandas(ds_from_strings)\n","\n","\n","    message = \"Descargando el modelo desde Hugging Face...\"\n","    self.update_state(state='PROGRESS', meta={'current': 30, 'total': 100, 'status': message})\n","\n","    model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n","\n","    compute_dtype = getattr(torch, \"float16\")\n","\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_quant_type=\"nf4\",\n","    )\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","      model_name,\n","      quantization_config=bnb_config,\n","      device_map={\"\": 0},\n","    )\n","    model.config.use_cache = False\n","    model.config.pretraining_tp = 1\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    peft_params = LoraConfig(\n","      r=64,\n","      lora_alpha=64,\n","      lora_dropout=0.1,\n","      bias=\"none\",\n","      task_type=\"CAUSAL_LM\",\n","    )\n","\n","\n","    message = \"Ajustando los parámetros...\"\n","    self.update_state(state='PROGRESS', meta={'current': 40, 'total': 100, 'status': message})\n","\n","    training_params = TrainingArguments(\n","      output_dir=\"./tmp/results\",\n","      num_train_epochs=3,\n","      per_device_train_batch_size=4,\n","      gradient_accumulation_steps=1,\n","      optim=\"paged_adamw_32bit\",\n","      save_steps=0,\n","      logging_steps=5,\n","      learning_rate=5e-5,\n","      weight_decay=0.001,\n","      fp16=False,\n","      bf16=False,\n","      max_grad_norm=0.3,\n","      max_steps=-1,\n","      warmup_ratio=0.03,\n","      group_by_length=True,\n","      lr_scheduler_type=\"linear\",\n","      report_to=\"tensorboard\",\n","    )\n","\n","    trainer = SFTTrainer(\n","      model=model,\n","      tokenizer=tokenizer,\n","      args=training_params,\n","      peft_config=peft_params,\n","      train_dataset=ds,\n","      dataset_text_field=\"text\",\n","      max_seq_length=None,\n","      packing=False,\n","    )\n","\n","    trainer.train()\n","    trainer.model.save_pretrained(\"./tmp/new_model\")\n","    trainer.tokenizer.save_pretrained(\"./tmp/new_model\")\n","\n","    del model\n","    del tokenizer\n","    del trainer\n","    torch.cuda.empty_cache()\n","\n","\n","    message = \"Combinando los parámetros...\"\n","    self.update_state(state='PROGRESS', meta={'current': 70, 'total': 100, 'status': message})\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","      model_name,\n","      low_cpu_mem_usage=True,\n","      return_dict=True,\n","      torch_dtype=torch.float16,\n","      device_map={\"\": 0},\n","    )\n","\n","    model = PeftModel.from_pretrained(base_model, \"./tmp/new_model\")\n","    model = model.merge_and_unload()\n","\n","\n","    message = \"Enviando el modelo ajustado a Hugging Face...\"\n","    self.update_state(state='PROGRESS', meta={'current': 90, 'total': 100, 'status': message})\n","\n","    HF_API_KEY = \"HUGGING_FACE_API_KEY_HERE\"\n","    subprocess.run([\"huggingface-cli\", \"login\", \"--token\", HF_API_KEY])\n","    model.push_to_hub(fine_tuned_model_name, max_shard_size=\"1000MB\", private=True)\n","    tokenizer.push_to_hub(fine_tuned_model_name, private=True)\n","\n","\n","    message = \"¡Entrenamiento completado!\"\n","    self.update_state(state='FINISH', meta={'current': 100, 'total': 100, 'status': message, 'result': 1003})\n","\n","    del model\n","    del tokenizer\n","    torch.cuda.empty_cache()\n","    comand = \"rm -r ~/.cache/huggingface/hub/\"\n","    subprocess.run(comand, shell=True)\n","    comand = \"rm -r ./tmp\"\n","    subprocess.run(comand, shell=True)\n","\n","\n","    return {'current': 100, 'total': 100, 'status': '¡Entrenamiento completado!', 'result': 1003}\n","\n","@celery.task(bind=True)\n","def download_inference(self, model_name):\n","    \"\"\" Esta funcion se encarga de descargar el LLM ajustado y de preparar la\n","    comunicacion entre el usuario y el LLM. \"\"\"\n","\n","    global tokenizer, model, pipe, llm, memory, template_conversation, prompt_conversation, conversation\n","\n","\n","    message = \"Conectando con Hugging Face...\"\n","    self.update_state(state='PROGRESS', meta={'current': 0, 'total': 100, 'status': message})\n","\n","    HF_API_KEY = \"HUGGING_FACE_API_KEY_HERE\"\n","    subprocess.run([\"huggingface-cli\", \"login\", \"--token\", HF_API_KEY])\n","\n","\n","    message = \"Descargando el LLM elegido...\"\n","    self.update_state(state='PROGRESS', meta={'current': 10, 'total': 100, 'status': message})\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        trust_remote_code=True,\n","        low_cpu_mem_usage=True,\n","        return_dict=True,\n","        torch_dtype=torch.float16,\n","        device_map={\"\": 0},\n","    )\n","\n","\n","    message=\"Preparando la comunicación con el LLM ajustado...\"\n","    self.update_state(state='PROGRESS', meta={'current': 80, 'total': 100, 'status': message})\n","\n","    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n","    llm = HuggingFacePipeline(pipeline=pipe, model_id=model_name, pipeline_kwargs={\"max_new_tokens\":200})\n","    memory = ConversationBufferMemory()\n","    template_conversation = \"\"\"{history}{input}\"\"\"\n","    prompt_conversation = PromptTemplate(input_variables=[\"history\", \"input\"], template=template_conversation)\n","    conversation = ConversationChain(\n","      prompt=prompt_conversation,\n","      llm=llm,\n","      verbose=False,\n","      memory=memory,\n","    )\n","\n","\n","    message=\"¡Modelo listo para recibir preguntas!\"\n","    self.update_state(state='FINISH', meta={'current': 100, 'total': 100, 'status': message, 'result': 1001})\n","\n","\n","    return {'current': 100, 'total': 100, 'status': '¡Modelo listo para recibir preguntas!', 'result': 1001}\n","\n","@celery.task(bind=True)\n","def run_inference(self, question):\n","    \"\"\" Esta funcion se encarga de adaptar las preguntas al formato adecuado\n","    y de generar las respuestas. \"\"\"\n","\n","    global conversation\n","\n","\n","    message = \"Realizando la pregunta...\"\n","    self.update_state(state='PROGRESS', meta={'current': 0, 'total': 100, 'status': message})\n","\n","    template = \"\"\"Below is an instruction that describes a task paired with input that provides further context. Write a response that appropriately completes the request.\n","\n","    ### Instruction:\n","    Answer the following question.\n","\n","    ### Input:\n","    {question}\n","\n","    ### Response:\n","    \"\"\"\n","\n","    prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n","    input = prompt.format(question=question)\n","    full_response = conversation.predict(input=input)\n","\n","    response_beginning = full_response.find(\"### Response:\")\n","    response_end = full_response.find(\"</s>\")\n","    response = full_response[response_beginning:response_end].replace(\"### Response:\", \"\").strip()\n","\n","    memory.clear()\n","\n","\n","    message = \"¡Respuesta generada!\"\n","    self.update_state(state='FINISH', meta={'current': 100, 'total': 100, 'status': message, 'result': response})\n","\n","\n","    return {'current': 100, 'total': 100, 'status': '¡Respuesta generada!', 'result': response}\n","\n","@celery.task(bind=True)\n","def end_inference(self):\n","    \"\"\" Esta funcion se encarga de liberar los recursos del servidor cuando\n","    el usuario no quiere realizar mas preguntas. \"\"\"\n","\n","    global tokenizer, model, pipe, llm, memory, template_conversation, prompt_conversation, conversation\n","\n","\n","    message = \"Cerrando la inferencia...\"\n","    self.update_state(state='PROGRESS', meta={'current': 0, 'total': 100, 'status': message})\n","\n","    del conversation\n","    del prompt_conversation\n","    del template_conversation\n","    del memory\n","    del llm\n","    del pipe\n","    del model\n","    del tokenizer\n","    torch.cuda.empty_cache()\n","    comand = \"rm -r ~/.cache/huggingface/hub/\"\n","    subprocess.run(comand, shell=True)\n","\n","\n","    message = \"¡Inferencia finalizada!\"\n","    self.update_state(state='FINISH', meta={'current': 100, 'total': 100, 'status': message, 'result': 1002})\n","\n","\n","    return {'current': 100, 'total': 100, 'status': '¡Inferencia finalizada!', 'result': 1002}\n","\n","\n","@app.route(\"/\", methods=[\"GET\"])\n","def main():\n","    \"\"\" Esta funcion carga la pagina principal. \"\"\"\n","    return render_template(\"main.html\")\n","\n","@app.route(\"/run_inference_page.html\", methods=[\"GET\"])\n","def run_inference_page():\n","    \"\"\" Esta funcion carga la pagina para conversar con el modelo. \"\"\"\n","    return render_template(\"run_inference_page.html\")\n","\n","@app.route(\"/downloadinference\", methods=[\"POST\"])\n","def downloadinference():\n","    \"\"\" Esta funcion se encarga de descargar el LLM ajustado y de preparar la\n","    comunicacion entre el usuario y el LLM. \"\"\"\n","    model_name = str(request.form.get(\"llm_name\"))\n","    task = download_inference.apply_async(args=[model_name])\n","\n","    return jsonify({}), 202, {'Location': url_for('taskstatus', task_id=task.id)}\n","\n","@app.route(\"/runinference\", methods=[\"POST\"])\n","def runinference():\n","    \"\"\" Esta funcion se encarga de adaptar las preguntas al formato adecuado\n","    y de generar las respuestas. \"\"\"\n","    question = str(request.form.get(\"question\"))\n","    task = run_inference.apply_async(args=[question])\n","\n","    return jsonify({}), 202, {'Location': url_for('taskstatus', task_id=task.id)}\n","\n","@app.route(\"/endinference\", methods=[\"POST\"])\n","def endinference():\n","    \"\"\" Esta funcion se encarga de liberar los recursos del servidor cuando\n","    el usuario no quiere realizar mas preguntas. \"\"\"\n","    task = end_inference.apply_async()\n","\n","    return jsonify({}), 202, {'Location': url_for('taskstatus', task_id=task.id)}\n","\n","@app.route(\"/fine_tuning_page.html\", methods=[\"GET\"])\n","def fine_tune_page():\n","    \"\"\" Esta funcion carga la pagina para ajustar el modelo. \"\"\"\n","    return render_template(\"fine_tuning_page.html\")\n","\n","@app.route(\"/finetunellm\", methods=[\"POST\"])\n","def finetunellm():\n","    \"\"\" Esta funcion se encarga del fine-tuning. Mientras se realiza el ajuste,\n","    se envian mensajes sobre el estado del proceso al cliente. \"\"\"\n","    ds_received = request.files[\"dataset\"]\n","    ds_csv = pd.read_csv(ds_received)\n","    if (len(ds_csv)>300):\n","      ds_csv = ds_csv.sample(n=300)\n","    adjust_ds_string = csv_to_string(ds_csv)\n","    new_model = str(request.form.get(\"llm_name\"))\n","    task = fine_tune_llm.apply_async(args=[adjust_ds_string, new_model])\n","\n","    return jsonify({}), 202, {'Location': url_for('taskstatus', task_id=task.id)}\n","\n","@app.route(\"/status/<task_id>\")\n","def taskstatus(task_id):\n","    \"\"\" Esta funcion envia actualizaciones sobre el estado de la tarea. \"\"\"\n","    try:\n","      task = fine_tune_llm.AsyncResult(task_id)\n","    except:\n","      pass\n","    try:\n","      task = download_inference.AsyncResult(task_id)\n","    except:\n","      pass\n","    try:\n","      task = run_inference.AsyncResult(task_id)\n","    except:\n","      pass\n","    try:\n","      task = end_inference.AsyncResult(task_id)\n","    except:\n","      pass\n","\n","    if task.state=='PENDING':\n","      response = {\n","        'state': task.state,\n","        'current': 0,\n","        'total': 1,\n","        'status': 'Iniciando...'\n","      }\n","    elif task.state=='FINISH':\n","      response = {\n","        'state': task.state,\n","        'current': task.info.get('current', 0),\n","        'total': task.info.get('total', 1),\n","        'status': task.info.get('status', ''),\n","        'result': task.info.get('result', ''),\n","      }\n","    elif task.state=='PROGRESS':\n","      response = {\n","        'state': task.state,\n","        'current': task.info.get('current', 0),\n","        'total': task.info.get('total', 1),\n","        'status': task.info.get('status', ''),\n","      }\n","    else:\n","      response = {\n","        'state': task.state,\n","        'current': 1,\n","        'total': 1,\n","        'status': str(task.info),\n","      }\n","    return jsonify(response)\n","\n","\n","if __name__ == \"__main__\":\n","    \"\"\" Esta funcion se encarga de ejecutar la aplicacion Flask. \"\"\"\n","    app.run()"],"metadata":{"id":"4U0qUSr8hcFU"},"execution_count":null,"outputs":[]}]}